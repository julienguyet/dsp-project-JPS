{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate the csv files. You can skip the first two cells if you already have the 'walmart_sales.csv' file in your data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"../../group_project/features.csv\")\n",
    "stores = pd.read_csv(\"../../group_project/stores.csv\")\n",
    "df = pd.read_csv(\"../../group_project/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(df.drop(columns=['IsHoliday']), features, on=['Store', 'Date'])\n",
    "full_data = pd.merge(data, stores, on=['Store'])\n",
    "output_path = '../data/full_data/walmart_sales.csv'\n",
    "# full_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a train and test set from the walmart_sales.csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_train_test(input_file, train_ratio=0.8):\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "\n",
    "        column_titles = f.readline().strip().split(',')\n",
    "        content = f.read()\n",
    "        file_size = len(content)\n",
    "        train_size = int(file_size * train_ratio)\n",
    "\n",
    "        # Create directory to store the split files\n",
    "        output_dir = '../data/full_data'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Write train and test parts into separate CSV files\n",
    "        train_content = content[:train_size]\n",
    "        test_content = content[train_size:]\n",
    "        train_filename = os.path.join(output_dir, 'train.csv')\n",
    "        test_filename = os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "        # Write train CSV file\n",
    "        with open(train_filename, 'w', newline='') as train_file:\n",
    "            writer = csv.writer(train_file)\n",
    "            writer.writerow(column_titles)\n",
    "            train_file.write(train_content)\n",
    "\n",
    "        # Write test CSV file\n",
    "        with open(test_filename, 'w', newline='') as test_file:\n",
    "            writer = csv.writer(test_file)\n",
    "            writer.writerow(column_titles)\n",
    "            test_file.write(test_content)\n",
    "\n",
    "        print('Train and test files created successfully.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test files created successfully.\n"
     ]
    }
   ],
   "source": [
    "split_file_train_test('../data/full_data/walmart_sales.csv', train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the train.csv file to train the model. Now, we can fragment the test file into multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(input_file, num_parts):\n",
    "    \n",
    "    with open(input_file, 'r') as f:\n",
    "\n",
    "        column_titles = f.readline().strip().split(',')\n",
    "        content = f.read()\n",
    "        file_size = len(content)\n",
    "        part_size = file_size // num_parts\n",
    "\n",
    "        # Create directory to store files\n",
    "        output_dir = '../data/raw_data'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Write each part into separate CSV files\n",
    "        for i in range(num_parts):\n",
    "            start_index = i * part_size\n",
    "            end_index = start_index + part_size\n",
    "            if i == num_parts - 1:  # Last part might be larger if file_size is not divisible by num_parts\n",
    "                end_index = file_size\n",
    "            \n",
    "            part_content = content[start_index:end_index]\n",
    "            part_filename = os.path.join(output_dir, f'test_part_{i + 1}.csv')\n",
    "            with open(part_filename, 'wb') as part_file:\n",
    "                part_file.write(','.join(column_titles).encode('utf-8') + b'\\n')\n",
    "                part_file.write(part_content.encode('utf-8'))\n",
    "\n",
    "        print(f'{num_parts} parts created successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85330 entries, 0 to 85329\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Store         85330 non-null  int64  \n",
      " 1   Dept          85329 non-null  float64\n",
      " 2   Date          85329 non-null  object \n",
      " 3   Weekly_Sales  85329 non-null  float64\n",
      " 4   Temperature   85329 non-null  float64\n",
      " 5   Fuel_Price    85329 non-null  float64\n",
      " 6   MarkDown1     32340 non-null  float64\n",
      " 7   MarkDown2     18477 non-null  float64\n",
      " 8   MarkDown3     27956 non-null  object \n",
      " 9   MarkDown4     20878 non-null  object \n",
      " 10  MarkDown5     32894 non-null  float64\n",
      " 11  CPI           85329 non-null  float64\n",
      " 12  Unemployment  85329 non-null  float64\n",
      " 13  IsHoliday     85329 non-null  object \n",
      " 14  Type          85329 non-null  object \n",
      " 15  Size          85329 non-null  float64\n",
      "dtypes: float64(10), int64(1), object(5)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# chekcing number of rows in test set\n",
    "test_data = pd.read_csv('../data/full_data/test.csv')\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 parts created successfully.\n"
     ]
    }
   ],
   "source": [
    "# creating a 100 files\n",
    "split_file('../data/full_data/test.csv', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selecting a file and delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_part_38.csv\n"
     ]
    }
   ],
   "source": [
    "directory = '../data/raw_data'\n",
    "print(random.choice(os.listdir(directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_part_4.csv\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(os.listdir(directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_part_23.csv\n"
     ]
    }
   ],
   "source": [
    "random_file = random.choice(os.listdir(directory))\n",
    "print(random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw_data/test_part_23.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = directory + \"/\" + random_file\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(\"File deleted successfully\")\n",
    "else:\n",
    "    print(\"Error: %s file not found\" % file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying data validation technique inside previous loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce46a51603574b0a92fc2ae922ffdaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dafb512194c447ebfcddefb7ce6b76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c537ad62f14c5b99409f796ada6156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory = '../data/raw_data'\n",
    "random_file = random.choice(os.listdir(directory))\n",
    "file_path = directory + \"/\" + random_file\n",
    "\n",
    "# creating a context\n",
    "context = gx.get_context()\n",
    "validator = context.sources.pandas_default.read_csv(file_path)\n",
    "\n",
    "# creating two expectation and saving them to the validator\n",
    "validator.expect_table_columns_to_match_ordered_list([\"Store\",\"Dept\",\"Date\",\"Weekly_Sales\",\"Temperature\",\"Fuel_Price\",\n",
    "                                                    \"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\",\"CPI\",\n",
    "                                                    \"Unemployment\",\"IsHoliday\",\"Type\",\"Size\"])\n",
    "validator.expect_column_values_to_not_be_null(\"Date\")\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "# creating a checkpoint to \n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"my_test_checkpoint\",\n",
    "    validator=validator,\n",
    ")\n",
    "checkpoint_result = checkpoint.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"run_id\": {\n",
       "    \"run_name\": null,\n",
       "    \"run_time\": \"2024-03-22T16:44:45.227389+01:00\"\n",
       "  },\n",
       "  \"run_results\": {\n",
       "    \"ValidationResultIdentifier::default/__none__/20240322T154445.227389Z/default_pandas_datasource-#ephemeral_pandas_asset\": {\n",
       "      \"validation_result\": {\n",
       "        \"success\": true,\n",
       "        \"results\": [\n",
       "          {\n",
       "            \"success\": true,\n",
       "            \"expectation_config\": {\n",
       "              \"expectation_type\": \"expect_table_columns_to_match_ordered_list\",\n",
       "              \"kwargs\": {\n",
       "                \"column_list\": [\n",
       "                  \"Store\",\n",
       "                  \"Dept\",\n",
       "                  \"Date\",\n",
       "                  \"Weekly_Sales\",\n",
       "                  \"Temperature\",\n",
       "                  \"Fuel_Price\",\n",
       "                  \"MarkDown1\",\n",
       "                  \"MarkDown2\",\n",
       "                  \"MarkDown3\",\n",
       "                  \"MarkDown4\",\n",
       "                  \"MarkDown5\",\n",
       "                  \"CPI\",\n",
       "                  \"Unemployment\",\n",
       "                  \"IsHoliday\",\n",
       "                  \"Type\",\n",
       "                  \"Size\"\n",
       "                ],\n",
       "                \"batch_id\": \"default_pandas_datasource-#ephemeral_pandas_asset\"\n",
       "              },\n",
       "              \"meta\": {}\n",
       "            },\n",
       "            \"result\": {\n",
       "              \"observed_value\": [\n",
       "                \"Store\",\n",
       "                \"Dept\",\n",
       "                \"Date\",\n",
       "                \"Weekly_Sales\",\n",
       "                \"Temperature\",\n",
       "                \"Fuel_Price\",\n",
       "                \"MarkDown1\",\n",
       "                \"MarkDown2\",\n",
       "                \"MarkDown3\",\n",
       "                \"MarkDown4\",\n",
       "                \"MarkDown5\",\n",
       "                \"CPI\",\n",
       "                \"Unemployment\",\n",
       "                \"IsHoliday\",\n",
       "                \"Type\",\n",
       "                \"Size\"\n",
       "              ]\n",
       "            },\n",
       "            \"meta\": {},\n",
       "            \"exception_info\": {\n",
       "              \"raised_exception\": false,\n",
       "              \"exception_traceback\": null,\n",
       "              \"exception_message\": null\n",
       "            }\n",
       "          },\n",
       "          {\n",
       "            \"success\": true,\n",
       "            \"expectation_config\": {\n",
       "              \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
       "              \"kwargs\": {\n",
       "                \"column\": \"Date\",\n",
       "                \"batch_id\": \"default_pandas_datasource-#ephemeral_pandas_asset\"\n",
       "              },\n",
       "              \"meta\": {}\n",
       "            },\n",
       "            \"result\": {\n",
       "              \"element_count\": 898,\n",
       "              \"unexpected_count\": 0,\n",
       "              \"unexpected_percent\": 0.0,\n",
       "              \"partial_unexpected_list\": [],\n",
       "              \"partial_unexpected_counts\": [],\n",
       "              \"partial_unexpected_index_list\": []\n",
       "            },\n",
       "            \"meta\": {},\n",
       "            \"exception_info\": {\n",
       "              \"raised_exception\": false,\n",
       "              \"exception_traceback\": null,\n",
       "              \"exception_message\": null\n",
       "            }\n",
       "          }\n",
       "        ],\n",
       "        \"evaluation_parameters\": {},\n",
       "        \"statistics\": {\n",
       "          \"evaluated_expectations\": 2,\n",
       "          \"successful_expectations\": 2,\n",
       "          \"unsuccessful_expectations\": 0,\n",
       "          \"success_percent\": 100.0\n",
       "        },\n",
       "        \"meta\": {\n",
       "          \"great_expectations_version\": \"0.18.12\",\n",
       "          \"expectation_suite_name\": \"default\",\n",
       "          \"run_id\": {\n",
       "            \"run_name\": null,\n",
       "            \"run_time\": \"2024-03-22T16:44:45.227389+01:00\"\n",
       "          },\n",
       "          \"batch_spec\": {\n",
       "            \"reader_method\": \"read_csv\",\n",
       "            \"reader_options\": {\n",
       "              \"filepath_or_buffer\": \"../data/raw_data/test_part_89.csv\"\n",
       "            }\n",
       "          },\n",
       "          \"batch_markers\": {\n",
       "            \"ge_load_time\": \"20240322T154445.233300Z\",\n",
       "            \"pandas_data_fingerprint\": \"c894ec9f7ee41749a6f8cf4726c65d5f\"\n",
       "          },\n",
       "          \"active_batch_definition\": {\n",
       "            \"datasource_name\": \"default_pandas_datasource\",\n",
       "            \"data_connector_name\": \"fluent\",\n",
       "            \"data_asset_name\": \"#ephemeral_pandas_asset\",\n",
       "            \"batch_identifiers\": {}\n",
       "          },\n",
       "          \"validation_time\": \"20240322T154445.245221Z\",\n",
       "          \"checkpoint_name\": \"my_test_checkpoint\",\n",
       "          \"validation_id\": null,\n",
       "          \"checkpoint_id\": null\n",
       "        }\n",
       "      },\n",
       "      \"actions_results\": {\n",
       "        \"store_validation_result\": {\n",
       "          \"class\": \"StoreValidationResultAction\"\n",
       "        },\n",
       "        \"store_evaluation_params\": {\n",
       "          \"class\": \"StoreEvaluationParametersAction\"\n",
       "        },\n",
       "        \"update_data_docs\": {\n",
       "          \"local_site\": \"file:///Users/julien/Documents/EPITA/S2/DSP/dsp-project-JPS/gx/uncommitted/data_docs/local_site/validations/default/__none__/20240322T154445.227389Z/default_pandas_datasource-%23ephemeral_pandas_asset.html\",\n",
       "          \"class\": \"UpdateDataDocsAction\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  \"checkpoint_config\": {\n",
       "    \"expectation_suite_name\": null,\n",
       "    \"site_names\": null,\n",
       "    \"name\": \"my_test_checkpoint\",\n",
       "    \"run_name_template\": null,\n",
       "    \"expectation_suite_ge_cloud_id\": null,\n",
       "    \"profilers\": [],\n",
       "    \"evaluation_parameters\": {},\n",
       "    \"action_list\": [\n",
       "      {\n",
       "        \"name\": \"store_validation_result\",\n",
       "        \"action\": {\n",
       "          \"class_name\": \"StoreValidationResultAction\"\n",
       "        }\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"store_evaluation_params\",\n",
       "        \"action\": {\n",
       "          \"class_name\": \"StoreEvaluationParametersAction\"\n",
       "        }\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"update_data_docs\",\n",
       "        \"action\": {\n",
       "          \"class_name\": \"UpdateDataDocsAction\"\n",
       "        }\n",
       "      }\n",
       "    ],\n",
       "    \"notify_with\": null,\n",
       "    \"template_name\": null,\n",
       "    \"batch_request\": {},\n",
       "    \"notify_on\": null,\n",
       "    \"default_validation_id\": null,\n",
       "    \"slack_webhook\": null,\n",
       "    \"runtime_configuration\": {},\n",
       "    \"ge_cloud_id\": null,\n",
       "    \"class_name\": \"Checkpoint\",\n",
       "    \"module_name\": \"great_expectations.checkpoint\",\n",
       "    \"validations\": [\n",
       "      {\n",
       "        \"expectation_suite_name\": \"default\",\n",
       "        \"batch_request\": {\n",
       "          \"datasource_name\": \"default_pandas_datasource\",\n",
       "          \"data_asset_name\": \"#ephemeral_pandas_asset\",\n",
       "          \"options\": {},\n",
       "          \"batch_slice\": null\n",
       "        },\n",
       "        \"id\": null,\n",
       "        \"name\": null,\n",
       "        \"expectation_suite_ge_cloud_id\": null\n",
       "      }\n",
       "    ],\n",
       "    \"config_version\": 1.0\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([ValidationResultIdentifier::default/__none__/20240322T154445.227389Z/default_pandas_datasource-#ephemeral_pandas_asset])\n"
     ]
    }
   ],
   "source": [
    "validation_result = checkpoint_result['run_results']\n",
    "print(validation_result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total expectations evaluated: 2\n",
      "Successful expectations: 2\n",
      "Unsuccessful expectations: 0\n",
      "Success percentage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Access the key directly from the dictionary\n",
    "validation_key = list(validation_result.keys())[0]  # Get the first (and only) key\n",
    "statistics = validation_result[validation_key]['validation_result']['statistics']\n",
    "\n",
    "# Now you can proceed with extracting the statistics as before\n",
    "evaluated_expectations = statistics['evaluated_expectations']\n",
    "successful_expectations = statistics['successful_expectations']\n",
    "unsuccessful_expectations = statistics['unsuccessful_expectations']\n",
    "success_percent = statistics['success_percent']\n",
    "\n",
    "# Print the summary\n",
    "print(f\"Total expectations evaluated: {evaluated_expectations}\")\n",
    "print(f\"Successful expectations: {successful_expectations}\")\n",
    "print(f\"Unsuccessful expectations: {unsuccessful_expectations}\")\n",
    "print(f\"Success percentage: {success_percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw_data/test_part_40.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad82b7d38cc489a87d36e8184db0396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecc4dd6799d4b5d8aadabe5ae2fb7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc36a8a135124c12aa69cdfd7d159258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Randomly selecting the file\n",
    "directory = '../data/raw_data'\n",
    "random_file = random.choice(os.listdir(directory))\n",
    "file_path = directory + \"/\" + random_file\n",
    "print(file_path)\n",
    "\n",
    "# 2. Running the data validation job\n",
    "context = gx.get_context()\n",
    "validator = context.sources.pandas_default.read_csv(file_path)\n",
    "\n",
    "# creating two expectation and saving them to the validator\n",
    "validator.expect_table_columns_to_match_ordered_list([\"Store\",\"Dept\",\"Date\",\"Weekly_Sales\",\"Temperature\",\"Fuel_Price\",\n",
    "                                                    \"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\",\"CPI\",\n",
    "                                                    \"Unemployment\",\"IsHoliday\",\"Type\",\"Size\"])\n",
    "validator.expect_column_values_to_not_be_null(\"Date\")\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"my_test_checkpoint\",\n",
    "    validator=validator,\n",
    ")\n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "validation_result = checkpoint_result['run_results']\n",
    "\n",
    "# Access the key directly from the dictionary\n",
    "validation_key = list(validation_result.keys())[0]\n",
    "statistics = validation_result[validation_key]['validation_result']['statistics']\n",
    "\n",
    "# extracting the statistics\n",
    "evaluated_expectations = statistics['evaluated_expectations']\n",
    "successful_expectations = statistics['successful_expectations']\n",
    "unsuccessful_expectations = statistics['unsuccessful_expectations']\n",
    "success_percent = statistics['success_percent']\n",
    "\n",
    "validation_result = checkpoint_result['run_results'][validation_key]['validation_result']\n",
    "results = validation_result['results']\n",
    "\n",
    "\n",
    "# 3. Data Ingestion based on quality\n",
    "good_data_directory = '../data/good_data'\n",
    "bad_data_directory = '../data/bad_data'\n",
    "unexpected_rows = set()\n",
    "unexpected_values = set()\n",
    "\n",
    "if success_percent == 100.0:\n",
    "    shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "elif success_percent == 0.0:\n",
    "    shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "else:\n",
    "    for result in results:\n",
    "    # Check if the expectation failed\n",
    "        if not result['success']:\n",
    "\n",
    "            # Get the expectation type\n",
    "            expectation_type = result['expectation_config']['expectation_type']\n",
    "            column_name = result['expectation_config']['kwargs']['column']\n",
    "\n",
    "            # Get the unexpected values and corresponding rows\n",
    "            #unexpected_values = result['result']['partial_unexpected_list']\n",
    "            #unexpected_rows = result['result']['partial_unexpected_index_list']\n",
    "            unexpected_values.update(result['result']['partial_unexpected_list'])\n",
    "            unexpected_rows.update(result['result']['partial_unexpected_index_list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(unexpected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.view_validation_result(checkpoint_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
