{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate the csv files. You can skip the first two cells if you already have the 'walmart_sales.csv' file in your data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"../../group_project/features.csv\")\n",
    "stores = pd.read_csv(\"../../group_project/stores.csv\")\n",
    "df = pd.read_csv(\"../../group_project/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(df.drop(columns=['IsHoliday']), features, on=['Store', 'Date'])\n",
    "full_data = pd.merge(data, stores, on=['Store'])\n",
    "output_path = '../data/full_data/walmart_sales.csv'\n",
    "# full_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a train and test set from the walmart_sales.csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_train_test(input_file, train_ratio=0.8):\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "\n",
    "        column_titles = f.readline().strip().split(',')\n",
    "        content = f.read()\n",
    "        file_size = len(content)\n",
    "        train_size = int(file_size * train_ratio)\n",
    "\n",
    "        # Create directory to store the split files\n",
    "        output_dir = '../data/full_data'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Write train and test parts into separate CSV files\n",
    "        train_content = content[:train_size]\n",
    "        test_content = content[train_size:]\n",
    "        train_filename = os.path.join(output_dir, 'train.csv')\n",
    "        test_filename = os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "        # Write train CSV file\n",
    "        with open(train_filename, 'w', newline='') as train_file:\n",
    "            writer = csv.writer(train_file)\n",
    "            writer.writerow(column_titles)\n",
    "            train_file.write(train_content)\n",
    "\n",
    "        # Write test CSV file\n",
    "        with open(test_filename, 'w', newline='') as test_file:\n",
    "            writer = csv.writer(test_file)\n",
    "            writer.writerow(column_titles)\n",
    "            test_file.write(test_content)\n",
    "\n",
    "        print('Train and test files created successfully.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test files created successfully.\n"
     ]
    }
   ],
   "source": [
    "split_file_train_test('../data/full_data/walmart_sales.csv', train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the train.csv file to train the model. Now, we can fragment the test file into multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(input_file, num_parts):\n",
    "    \n",
    "    with open(input_file, 'r') as f:\n",
    "\n",
    "        column_titles = f.readline().strip().split(',')\n",
    "        content = f.read()\n",
    "        file_size = len(content)\n",
    "        part_size = file_size // num_parts\n",
    "\n",
    "        # Create directory to store files\n",
    "        output_dir = '../data/raw_data'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Write each part into separate CSV files\n",
    "        for i in range(num_parts):\n",
    "            start_index = i * part_size\n",
    "            end_index = start_index + part_size\n",
    "            if i == num_parts - 1:  # Last part might be larger if file_size is not divisible by num_parts\n",
    "                end_index = file_size\n",
    "            \n",
    "            part_content = content[start_index:end_index]\n",
    "            part_filename = os.path.join(output_dir, f'test_part_{i + 1}.csv')\n",
    "            with open(part_filename, 'wb') as part_file:\n",
    "                part_file.write(','.join(column_titles).encode('utf-8') + b'\\n')\n",
    "                part_file.write(part_content.encode('utf-8'))\n",
    "\n",
    "        print(f'{num_parts} parts created successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85330 entries, 0 to 85329\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Store         85330 non-null  int64  \n",
      " 1   Dept          85329 non-null  float64\n",
      " 2   Date          85329 non-null  object \n",
      " 3   Weekly_Sales  85329 non-null  float64\n",
      " 4   Temperature   85329 non-null  float64\n",
      " 5   Fuel_Price    85329 non-null  float64\n",
      " 6   MarkDown1     32340 non-null  float64\n",
      " 7   MarkDown2     18477 non-null  float64\n",
      " 8   MarkDown3     27956 non-null  object \n",
      " 9   MarkDown4     20878 non-null  object \n",
      " 10  MarkDown5     32894 non-null  float64\n",
      " 11  CPI           85329 non-null  float64\n",
      " 12  Unemployment  85329 non-null  float64\n",
      " 13  IsHoliday     85329 non-null  object \n",
      " 14  Type          85329 non-null  object \n",
      " 15  Size          85329 non-null  float64\n",
      "dtypes: float64(10), int64(1), object(5)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# chekcing number of rows in test set\n",
    "test_data = pd.read_csv('../data/full_data/test.csv')\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 parts created successfully.\n"
     ]
    }
   ],
   "source": [
    "# creating a 100 files\n",
    "split_file('../data/full_data/test.csv', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selecting a file and delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_part_78.csv\n"
     ]
    }
   ],
   "source": [
    "directory = '../data/raw_data'\n",
    "print(random.choice(os.listdir(directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_part_18.csv\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(os.listdir(directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_part_77.csv\n"
     ]
    }
   ],
   "source": [
    "random_file = random.choice(os.listdir(directory))\n",
    "print(random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw_data/test_part_77.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = directory + \"/\" + random_file\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(\"File deleted successfully\")\n",
    "else:\n",
    "    print(\"Error: %s file not found\" % file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying data validation technique inside previous loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1057c5ebea8d44eda72775a065f05334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5639e5071ddf4128a716781054c80261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14852eced2d34c88a647297a63d72d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory = '../data/raw_data'\n",
    "random_file = random.choice(os.listdir(directory))\n",
    "file_path = directory + \"/\" + random_file\n",
    "\n",
    "# creating a context\n",
    "context = gx.get_context()\n",
    "validator = context.sources.pandas_default.read_csv(file_path)\n",
    "\n",
    "# creating two expectation and saving them to the validator\n",
    "validator.expect_table_columns_to_match_ordered_list([\"Store\",\"Dept\",\"Date\",\"Weekly_Sales\",\"Temperature\",\"Fuel_Price\",\n",
    "                                                    \"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\",\"CPI\",\n",
    "                                                    \"Unemployment\",\"IsHoliday\",\"Type\",\"Size\"])\n",
    "validator.expect_column_values_to_not_be_null(\"Date\")\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "# creating a checkpoint to \n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"my_test_checkpoint\",\n",
    "    validator=validator,\n",
    ")\n",
    "checkpoint_result = checkpoint.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"run_id\": {\n",
       "    \"run_name\": null,\n",
       "    \"run_time\": \"2024-03-22T16:44:45.227389+01:00\"\n",
       "  },\n",
       "  \"run_results\": {\n",
       "    \"ValidationResultIdentifier::default/__none__/20240322T154445.227389Z/default_pandas_datasource-#ephemeral_pandas_asset\": {\n",
       "      \"validation_result\": {\n",
       "        \"success\": true,\n",
       "        \"results\": [\n",
       "          {\n",
       "            \"success\": true,\n",
       "            \"expectation_config\": {\n",
       "              \"expectation_type\": \"expect_table_columns_to_match_ordered_list\",\n",
       "              \"kwargs\": {\n",
       "                \"column_list\": [\n",
       "                  \"Store\",\n",
       "                  \"Dept\",\n",
       "                  \"Date\",\n",
       "                  \"Weekly_Sales\",\n",
       "                  \"Temperature\",\n",
       "                  \"Fuel_Price\",\n",
       "                  \"MarkDown1\",\n",
       "                  \"MarkDown2\",\n",
       "                  \"MarkDown3\",\n",
       "                  \"MarkDown4\",\n",
       "                  \"MarkDown5\",\n",
       "                  \"CPI\",\n",
       "                  \"Unemployment\",\n",
       "                  \"IsHoliday\",\n",
       "                  \"Type\",\n",
       "                  \"Size\"\n",
       "                ],\n",
       "                \"batch_id\": \"default_pandas_datasource-#ephemeral_pandas_asset\"\n",
       "              },\n",
       "              \"meta\": {}\n",
       "            },\n",
       "            \"result\": {\n",
       "              \"observed_value\": [\n",
       "                \"Store\",\n",
       "                \"Dept\",\n",
       "                \"Date\",\n",
       "                \"Weekly_Sales\",\n",
       "                \"Temperature\",\n",
       "                \"Fuel_Price\",\n",
       "                \"MarkDown1\",\n",
       "                \"MarkDown2\",\n",
       "                \"MarkDown3\",\n",
       "                \"MarkDown4\",\n",
       "                \"MarkDown5\",\n",
       "                \"CPI\",\n",
       "                \"Unemployment\",\n",
       "                \"IsHoliday\",\n",
       "                \"Type\",\n",
       "                \"Size\"\n",
       "              ]\n",
       "            },\n",
       "            \"meta\": {},\n",
       "            \"exception_info\": {\n",
       "              \"raised_exception\": false,\n",
       "              \"exception_traceback\": null,\n",
       "              \"exception_message\": null\n",
       "            }\n",
       "          },\n",
       "          {\n",
       "            \"success\": true,\n",
       "            \"expectation_config\": {\n",
       "              \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
       "              \"kwargs\": {\n",
       "                \"column\": \"Date\",\n",
       "                \"batch_id\": \"default_pandas_datasource-#ephemeral_pandas_asset\"\n",
       "              },\n",
       "              \"meta\": {}\n",
       "            },\n",
       "            \"result\": {\n",
       "              \"element_count\": 898,\n",
       "              \"unexpected_count\": 0,\n",
       "              \"unexpected_percent\": 0.0,\n",
       "              \"partial_unexpected_list\": [],\n",
       "              \"partial_unexpected_counts\": [],\n",
       "              \"partial_unexpected_index_list\": []\n",
       "            },\n",
       "            \"meta\": {},\n",
       "            \"exception_info\": {\n",
       "              \"raised_exception\": false,\n",
       "              \"exception_traceback\": null,\n",
       "              \"exception_message\": null\n",
       "            }\n",
       "          }\n",
       "        ],\n",
       "        \"evaluation_parameters\": {},\n",
       "        \"statistics\": {\n",
       "          \"evaluated_expectations\": 2,\n",
       "          \"successful_expectations\": 2,\n",
       "          \"unsuccessful_expectations\": 0,\n",
       "          \"success_percent\": 100.0\n",
       "        },\n",
       "        \"meta\": {\n",
       "          \"great_expectations_version\": \"0.18.12\",\n",
       "          \"expectation_suite_name\": \"default\",\n",
       "          \"run_id\": {\n",
       "            \"run_name\": null,\n",
       "            \"run_time\": \"2024-03-22T16:44:45.227389+01:00\"\n",
       "          },\n",
       "          \"batch_spec\": {\n",
       "            \"reader_method\": \"read_csv\",\n",
       "            \"reader_options\": {\n",
       "              \"filepath_or_buffer\": \"../data/raw_data/test_part_89.csv\"\n",
       "            }\n",
       "          },\n",
       "          \"batch_markers\": {\n",
       "            \"ge_load_time\": \"20240322T154445.233300Z\",\n",
       "            \"pandas_data_fingerprint\": \"c894ec9f7ee41749a6f8cf4726c65d5f\"\n",
       "          },\n",
       "          \"active_batch_definition\": {\n",
       "            \"datasource_name\": \"default_pandas_datasource\",\n",
       "            \"data_connector_name\": \"fluent\",\n",
       "            \"data_asset_name\": \"#ephemeral_pandas_asset\",\n",
       "            \"batch_identifiers\": {}\n",
       "          },\n",
       "          \"validation_time\": \"20240322T154445.245221Z\",\n",
       "          \"checkpoint_name\": \"my_test_checkpoint\",\n",
       "          \"validation_id\": null,\n",
       "          \"checkpoint_id\": null\n",
       "        }\n",
       "      },\n",
       "      \"actions_results\": {\n",
       "        \"store_validation_result\": {\n",
       "          \"class\": \"StoreValidationResultAction\"\n",
       "        },\n",
       "        \"store_evaluation_params\": {\n",
       "          \"class\": \"StoreEvaluationParametersAction\"\n",
       "        },\n",
       "        \"update_data_docs\": {\n",
       "          \"local_site\": \"file:///Users/julien/Documents/EPITA/S2/DSP/dsp-project-JPS/gx/uncommitted/data_docs/local_site/validations/default/__none__/20240322T154445.227389Z/default_pandas_datasource-%23ephemeral_pandas_asset.html\",\n",
       "          \"class\": \"UpdateDataDocsAction\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  \"checkpoint_config\": {\n",
       "    \"expectation_suite_name\": null,\n",
       "    \"site_names\": null,\n",
       "    \"name\": \"my_test_checkpoint\",\n",
       "    \"run_name_template\": null,\n",
       "    \"expectation_suite_ge_cloud_id\": null,\n",
       "    \"profilers\": [],\n",
       "    \"evaluation_parameters\": {},\n",
       "    \"action_list\": [\n",
       "      {\n",
       "        \"name\": \"store_validation_result\",\n",
       "        \"action\": {\n",
       "          \"class_name\": \"StoreValidationResultAction\"\n",
       "        }\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"store_evaluation_params\",\n",
       "        \"action\": {\n",
       "          \"class_name\": \"StoreEvaluationParametersAction\"\n",
       "        }\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"update_data_docs\",\n",
       "        \"action\": {\n",
       "          \"class_name\": \"UpdateDataDocsAction\"\n",
       "        }\n",
       "      }\n",
       "    ],\n",
       "    \"notify_with\": null,\n",
       "    \"template_name\": null,\n",
       "    \"batch_request\": {},\n",
       "    \"notify_on\": null,\n",
       "    \"default_validation_id\": null,\n",
       "    \"slack_webhook\": null,\n",
       "    \"runtime_configuration\": {},\n",
       "    \"ge_cloud_id\": null,\n",
       "    \"class_name\": \"Checkpoint\",\n",
       "    \"module_name\": \"great_expectations.checkpoint\",\n",
       "    \"validations\": [\n",
       "      {\n",
       "        \"expectation_suite_name\": \"default\",\n",
       "        \"batch_request\": {\n",
       "          \"datasource_name\": \"default_pandas_datasource\",\n",
       "          \"data_asset_name\": \"#ephemeral_pandas_asset\",\n",
       "          \"options\": {},\n",
       "          \"batch_slice\": null\n",
       "        },\n",
       "        \"id\": null,\n",
       "        \"name\": null,\n",
       "        \"expectation_suite_ge_cloud_id\": null\n",
       "      }\n",
       "    ],\n",
       "    \"config_version\": 1.0\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([ValidationResultIdentifier::default/__none__/20240323T145907.870749Z/default_pandas_datasource-#ephemeral_pandas_asset])\n"
     ]
    }
   ],
   "source": [
    "validation_result = checkpoint_result['run_results']\n",
    "print(validation_result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total expectations evaluated: 2\n",
      "Successful expectations: 2\n",
      "Unsuccessful expectations: 0\n",
      "Success percentage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Access the key directly from the dictionary\n",
    "validation_key = list(validation_result.keys())[0]  # Get the first (and only) key\n",
    "statistics = validation_result[validation_key]['validation_result']['statistics']\n",
    "\n",
    "# Now you can proceed with extracting the statistics as before\n",
    "evaluated_expectations = statistics['evaluated_expectations']\n",
    "successful_expectations = statistics['successful_expectations']\n",
    "unsuccessful_expectations = statistics['unsuccessful_expectations']\n",
    "success_percent = statistics['success_percent']\n",
    "\n",
    "# Print the summary\n",
    "print(f\"Total expectations evaluated: {evaluated_expectations}\")\n",
    "print(f\"Successful expectations: {successful_expectations}\")\n",
    "print(f\"Unsuccessful expectations: {unsuccessful_expectations}\")\n",
    "print(f\"Success percentage: {success_percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw_data/test_part_93.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124aef0a70f242ceaa3c766d5abc5565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ff7a935add45debc4c3ccb531a0a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18993fc4cf7445489f7f0ae3d1f1e353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Randomly selecting the file\n",
    "directory = '../data/raw_data'\n",
    "random_file = random.choice(os.listdir(directory))\n",
    "file_path = directory + \"/\" + random_file\n",
    "print(file_path)\n",
    "\n",
    "# 2. Running the data validation job\n",
    "context = gx.get_context()\n",
    "validator = context.sources.pandas_default.read_csv(file_path)\n",
    "\n",
    "# creating two expectation and saving them to the validator\n",
    "validator.expect_table_columns_to_match_ordered_list([\"Store\",\"Dept\",\"Date\",\"Weekly_Sales\",\"Temperature\",\"Fuel_Price\",\n",
    "                                                    \"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\",\"CPI\",\n",
    "                                                    \"Unemployment\",\"IsHoliday\",\"Type\",\"Size\"])\n",
    "validator.expect_column_values_to_not_be_null(\"Date\")\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"my_test_checkpoint\",\n",
    "    validator=validator,\n",
    ")\n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "validation_result = checkpoint_result['run_results']\n",
    "\n",
    "# Access the key directly from the dictionary\n",
    "validation_key = list(validation_result.keys())[0]\n",
    "statistics = validation_result[validation_key]['validation_result']['statistics']\n",
    "\n",
    "# extracting the statistics\n",
    "evaluated_expectations = statistics['evaluated_expectations']\n",
    "successful_expectations = statistics['successful_expectations']\n",
    "unsuccessful_expectations = statistics['unsuccessful_expectations']\n",
    "success_percent = statistics['success_percent']\n",
    "\n",
    "validation_result = checkpoint_result['run_results'][validation_key]['validation_result']\n",
    "results = validation_result['results']\n",
    "\n",
    "\n",
    "# 3. Data Ingestion based on quality\n",
    "good_data_directory = '../data/good_data'\n",
    "bad_data_directory = '../data/bad_data'\n",
    "unexpected_rows = set()\n",
    "unexpected_values = set()\n",
    "\n",
    "if success_percent == 100.0:\n",
    "    shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "elif success_percent == 0.0:\n",
    "    shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "else:\n",
    "    for result in results:\n",
    "    # Check if the expectation failed\n",
    "        if not result['success']:\n",
    "\n",
    "            # Get the expectation type\n",
    "            expectation_type = result['expectation_config']['expectation_type']\n",
    "            column_name = result['expectation_config']['kwargs']['column']\n",
    "\n",
    "            # Get the unexpected values and corresponding rows\n",
    "            #unexpected_values = result['result']['partial_unexpected_list']\n",
    "            #unexpected_rows = result['result']['partial_unexpected_index_list']\n",
    "            unexpected_values.update(result['result']['partial_unexpected_list'])\n",
    "            unexpected_rows.update(result['result']['partial_unexpected_index_list'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with corrupted file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e362c5df4f4e20ab9deee3d8872e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0807c9ab0146edb88f079e33f2ffe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0472ba1d22c54d7f91dc53a07ef601d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# 1. Randomly selecting the file\n",
    "file_path = '../data/raw_data/test_part_1.csv'\n",
    "\n",
    "# 2. Running the data validation job\n",
    "context = gx.get_context()\n",
    "validator = context.sources.pandas_default.read_csv(file_path)\n",
    "\n",
    "# creating two expectation and saving them to the validator\n",
    "validator.expect_column_values_to_not_be_null(\"Date\")\n",
    "validator.expect_column_values_to_not_be_null(\"CPI\")\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"checkpoint1\",\n",
    "    validator=validator,\n",
    ")\n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "validation_result = checkpoint_result['run_results']\n",
    "\n",
    "# Access the key directly from the dictionary\n",
    "validation_key = list(validation_result.keys())[0]\n",
    "statistics = validation_result[validation_key]['validation_result']['statistics']\n",
    "\n",
    "# extracting the statistics\n",
    "evaluated_expectations = statistics['evaluated_expectations']\n",
    "successful_expectations = statistics['successful_expectations']\n",
    "unsuccessful_expectations = statistics['unsuccessful_expectations']\n",
    "success_percent = statistics['success_percent']\n",
    "\n",
    "validation_result = checkpoint_result['run_results'][validation_key]['validation_result']\n",
    "results = validation_result['results']\n",
    "\n",
    "\n",
    "# 3. Data Ingestion based on quality\n",
    "good_data_directory = '../data/good_data'\n",
    "bad_data_directory = '../data/bad_data'\n",
    "unexpected_values = set()\n",
    "unexpected_rows = set()\n",
    "\n",
    "if success_percent == 100.0:\n",
    "    shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "    print(f'File has no missing data and has been moved to {good_data_directory}')\n",
    "elif success_percent == 0.0:\n",
    "    shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "    print(f'File is fully corrupted and has been moved to {bad_data_directory}')\n",
    "else:\n",
    "    for result in results:\n",
    "    # Check if the expectation failed\n",
    "        if not result['success']:\n",
    "            # Get the expectation type\n",
    "            expectation_type = result['expectation_config']['expectation_type']\n",
    "            column_name = result['expectation_config']['kwargs']['column']\n",
    "\n",
    "            # Get the unexpected values and corresponding rows\n",
    "            #unexpected_values = result['result']['partial_unexpected_list']\n",
    "            #unexpected_rows = result['result']['partial_unexpected_index_list']\n",
    "            unexpected_values.update(result['result']['partial_unexpected_list'])\n",
    "            unexpected_rows.update(result['result']['partial_unexpected_index_list'])\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            good_data = df.copy()\n",
    "            bad_data = df.iloc[unexpected_rows]\n",
    "\n",
    "            ct = datetime.datetime.now()\n",
    "            ts = str(ct.timestamp())\n",
    "            file_path_good_data = good_data_directory + '/' + 'good_data_' + ts + '.csv'\n",
    "            file_path_bad_data = bad_data_directory + '/' + 'bad_data_' + ts + '.csv'\n",
    "\n",
    "            for rows in unexpected_rows:\n",
    "                good_data.drop(rows, inplace=True)\n",
    "            \n",
    "            good_data.to_csv(file_path_good_data) \n",
    "            bad_data.to_csv(file_path_bad_data)\n",
    "            print(f'Good data has been moved to: {file_path_good_data}')\n",
    "            print(f'Bad data has been moved to: {file_path_bad_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.view_validation_result(checkpoint_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import Checkpoint\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "def data_quality(directory):\n",
    "\n",
    "    # 1. Randomly selecting the file\n",
    "    random_file = random.choice(os.listdir(directory))\n",
    "    file_path = directory + \"/\" + random_file\n",
    "    print(f'Here is the selected file: {file_path}')\n",
    "\n",
    "    # 2. Running the data validation job\n",
    "    context = gx.get_context()\n",
    "    validator = context.sources.pandas_default.read_csv(file_path)\n",
    "\n",
    "    # creating two expectation and saving them to the validator\n",
    "    validator.expect_table_columns_to_match_ordered_list([\"Store\",\"Dept\",\"Date\",\"Weekly_Sales\",\"Temperature\",\"Fuel_Price\",\n",
    "                                                        \"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\",\"CPI\",\n",
    "                                                        \"Unemployment\",\"IsHoliday\",\"Type\",\"Size\"])\n",
    "    validator.expect_column_values_to_not_be_null(\"Date\")\n",
    "    validator.expect_column_values_to_not_be_null(\"CPI\")\n",
    "    validator.save_expectation_suite()\n",
    "\n",
    "    checkpoint = context.add_or_update_checkpoint(\n",
    "        name=\"my_test_checkpoint\",\n",
    "        validator=validator,\n",
    "    )\n",
    "    checkpoint_result = checkpoint.run()\n",
    "\n",
    "    validation_result = checkpoint_result['run_results']\n",
    "\n",
    "    # Access the key directly from the dictionary\n",
    "    validation_key = list(validation_result.keys())[0]\n",
    "    statistics = validation_result[validation_key]['validation_result']['statistics']\n",
    "\n",
    "    # extracting the statistics\n",
    "    evaluated_expectations = statistics['evaluated_expectations']\n",
    "    successful_expectations = statistics['successful_expectations']\n",
    "    unsuccessful_expectations = statistics['unsuccessful_expectations']\n",
    "    success_percent = statistics['success_percent']\n",
    "\n",
    "    validation_result = checkpoint_result['run_results'][validation_key]['validation_result']\n",
    "    results = validation_result['results']\n",
    "\n",
    "\n",
    "    # 3. Data Ingestion based on quality\n",
    "    good_data_directory = '../data/good_data'\n",
    "    bad_data_directory = '../data/bad_data'\n",
    "    unexpected_values = set()\n",
    "    unexpected_rows = set()\n",
    "\n",
    "    if success_percent == 100.0:\n",
    "        shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "        print(f'File has no missing data and has been moved to {good_data_directory}')\n",
    "    elif success_percent == 0.0:\n",
    "        shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "        print(f'File is fully corrupted and has been moved to {bad_data_directory}')\n",
    "    else:\n",
    "        for result in results:\n",
    "        # Check if the expectation failed\n",
    "            if not result['success']:\n",
    "                # Get the expectation type\n",
    "                expectation_type = result['expectation_config']['expectation_type']\n",
    "                column_name = result['expectation_config']['kwargs']['column']\n",
    "\n",
    "                # Get the unexpected values and corresponding rows\n",
    "                #unexpected_values = result['result']['partial_unexpected_list']\n",
    "                #unexpected_rows = result['result']['partial_unexpected_index_list']\n",
    "                unexpected_values.update(result['result']['partial_unexpected_list'])\n",
    "                unexpected_rows.update(result['result']['partial_unexpected_index_list'])\n",
    "\n",
    "                df = pd.read_csv(file_path)\n",
    "                good_data = df.copy()\n",
    "                bad_data = df.iloc[unexpected_rows]\n",
    "\n",
    "                ct = datetime.datetime.now()\n",
    "                ts = str(ct.timestamp())\n",
    "                file_path_good_data = good_data_directory + '/' + 'good_data_' + ts + '.csv'\n",
    "                file_path_bad_data = bad_data_directory + '/' + 'bad_data_' + ts + '.csv'\n",
    "\n",
    "                for rows in unexpected_rows:\n",
    "                    good_data.drop(rows, inplace=True)\n",
    "                \n",
    "                good_data.to_csv(file_path_good_data) \n",
    "                bad_data.to_csv(file_path_bad_data)\n",
    "                print(f'Good data has been moved to: {file_path_good_data}')\n",
    "                print(f'Bad data has been moved to: {file_path_bad_data}')\n",
    "                \n",
    "    \n",
    "    return unexpected_values, unexpected_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with dataframe instead of csv + checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "df = ge.read_csv('../data/raw_data/test_part_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"result\": {\n",
       "    \"element_count\": 861,\n",
       "    \"unexpected_count\": 1,\n",
       "    \"unexpected_percent\": 0.11614401858304298,\n",
       "    \"unexpected_percent_total\": 0.11614401858304298,\n",
       "    \"partial_unexpected_list\": []\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.expect_column_values_to_not_be_null(column='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"result\": {\n",
       "    \"element_count\": 861,\n",
       "    \"unexpected_count\": 1,\n",
       "    \"unexpected_percent\": 0.11614401858304298,\n",
       "    \"unexpected_percent_total\": 0.11614401858304298,\n",
       "    \"partial_unexpected_list\": []\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.expect_column_values_to_not_be_null(column='CPI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(0, len(df)):\n",
    "    temp_df = df.iloc[[i]]\n",
    "    date_quality = dict(temp_df.expect_column_values_to_not_be_null(column='Date'))\n",
    "    cpi_quality = dict(temp_df.expect_column_values_to_not_be_null(column='CPI'))\n",
    "\n",
    "    if date_quality[\"success\"] == False and i not in rows:\n",
    "        rows.append(i)\n",
    "\n",
    "    elif cpi_quality[\"success\"] == False and i not in rows:\n",
    "        rows.append(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good data has been moved to: ../data/good_data/good_data_1711406904.096826.csv\n",
      "Bad data has been moved to: ../data/bad_data/bad_data_1711406904.096826.csv\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import shutil\n",
    "import datetime\n",
    "ct = datetime.datetime.now()\n",
    "ts = str(ct.timestamp())\n",
    "good_data_directory = '../data/good_data'\n",
    "bad_data_directory = '../data/bad_data'\n",
    "file_path = '../data/raw_data/test_part_1.csv'\n",
    "file_path_good_data = good_data_directory + '/' + 'good_data_' + str(ts) + '.csv'\n",
    "file_path_bad_data = bad_data_directory + '/' + 'bad_data_' + str(ts) + '.csv'\n",
    "\n",
    "# complete loop to move file based on quality\n",
    "\n",
    "df = ge.read_csv('../data/raw_data/test_part_1.csv')\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    temp_df = df.iloc[[i]]\n",
    "    date_quality = dict(temp_df.expect_column_values_to_not_be_null(column='Date'))\n",
    "    cpi_quality = dict(temp_df.expect_column_values_to_not_be_null(column='CPI'))\n",
    "\n",
    "    if date_quality[\"success\"] == False and i not in rows:\n",
    "        rows.append(i)\n",
    "\n",
    "    elif cpi_quality[\"success\"] == False and i not in rows:\n",
    "        rows.append(i)\n",
    "    \n",
    "quality_ratio = len(rows) / len(df)\n",
    "\n",
    "if quality_ratio == 1:\n",
    "    shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "elif quality_ratio < 0.20:\n",
    "    good_data = df.drop(index=rows)\n",
    "    bad_data = df.iloc[rows]\n",
    "    good_data.to_csv(file_path_good_data) \n",
    "    bad_data.to_csv(file_path_bad_data)\n",
    "    print(f'Good data has been moved to: {file_path_good_data}')\n",
    "    print(f'Bad data has been moved to: {file_path_bad_data}')\n",
    "else:\n",
    "    shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with random file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the selected file: ../data/raw_data/test_part_37.csv\n",
      "Good data has been moved to: ../data/good_data/good_data_1711407428.85033.csv\n",
      "Bad data has been moved to: ../data/bad_data/bad_data_1711407428.85033.csv\n",
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import shutil\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "ct = datetime.datetime.now()\n",
    "ts = str(ct.timestamp())\n",
    "\n",
    "good_data_directory = '../data/good_data'\n",
    "bad_data_directory = '../data/bad_data'\n",
    "\n",
    "raw_data_directory = '../data/raw_data'\n",
    "random_file = random.choice(os.listdir(raw_data_directory))\n",
    "file_path = raw_data_directory + \"/\" + random_file\n",
    "print(f'Here is the selected file: {file_path}')\n",
    "\n",
    "file_path_good_data = good_data_directory + '/' + 'good_data_' + str(ts) + '.csv'\n",
    "file_path_bad_data = bad_data_directory + '/' + 'bad_data_' + str(ts) + '.csv'\n",
    "\n",
    "\n",
    "# complete loop to move file based on quality\n",
    "\n",
    "df = ge.read_csv(file_path)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    temp_df = df.iloc[[i]]\n",
    "    date_quality = dict(temp_df.expect_column_values_to_not_be_null(column='Date'))\n",
    "    cpi_quality = dict(temp_df.expect_column_values_to_not_be_null(column='CPI'))\n",
    "\n",
    "    if date_quality[\"success\"] == False and i not in rows:\n",
    "        rows.append(i)\n",
    "\n",
    "    elif cpi_quality[\"success\"] == False and i not in rows:\n",
    "        rows.append(i)\n",
    "    \n",
    "quality_ratio = len(rows) / len(df)\n",
    "\n",
    "if quality_ratio == 1:\n",
    "    shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "elif quality_ratio < 0.20:\n",
    "    good_data = df.drop(index=rows)\n",
    "    bad_data = df.iloc[rows]\n",
    "    good_data.to_csv(file_path_good_data) \n",
    "    bad_data.to_csv(file_path_bad_data)\n",
    "    print(f'Good data has been moved to: {file_path_good_data}')\n",
    "    print(f'Bad data has been moved to: {file_path_bad_data}')\n",
    "else:\n",
    "    shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(\"File deleted successfully\")\n",
    "else:\n",
    "    print(\"Error: %s file not found\" % file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "import shutil\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "def data_validation(raw_data_directory, good_data_directory, bad_data_directory):\n",
    "\n",
    "    ct = datetime.datetime.now()\n",
    "    ts = str(ct.timestamp())\n",
    "\n",
    "    random_file = random.choice(os.listdir(raw_data_directory))\n",
    "    file_path = raw_data_directory + \"/\" + random_file\n",
    "    print(f'Here is the selected file: {file_path}')\n",
    "\n",
    "    file_path_good_data = good_data_directory + '/' + 'good_data_' + str(ts) + '.csv'\n",
    "    file_path_bad_data = bad_data_directory + '/' + 'bad_data_' + str(ts) + '.csv'\n",
    "\n",
    "    # complete loop to move file based on quality\n",
    "\n",
    "    df = ge.read_csv(file_path)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        temp_df = df.iloc[[i]]\n",
    "        date_quality = dict(temp_df.expect_column_values_to_not_be_null(column='Date'))\n",
    "        cpi_quality = dict(temp_df.expect_column_values_to_not_be_null(column='CPI'))\n",
    "\n",
    "        if date_quality[\"success\"] == False and i not in rows:\n",
    "            rows.append(i)\n",
    "\n",
    "        elif cpi_quality[\"success\"] == False and i not in rows:\n",
    "            rows.append(i)\n",
    "    \n",
    "    corrupted_ratio = len(rows) / len(df)\n",
    "    print(f'Corrupted Ratio = {corrupted_ratio}')\n",
    "\n",
    "    if corrupted_ratio == 0:\n",
    "        shutil.move(file_path, os.path.join(good_data_directory, os.path.basename(file_path)))\n",
    "    elif corrupted_ratio < 0.20:\n",
    "        good_data = df.drop(index=rows)\n",
    "        bad_data = df.iloc[rows]\n",
    "        good_data.to_csv(file_path_good_data) \n",
    "        bad_data.to_csv(file_path_bad_data)\n",
    "        print(f'Good data has been moved to: {file_path_good_data}')\n",
    "        print(f'Bad data has been moved to: {file_path_bad_data}')\n",
    "    else:\n",
    "        shutil.move(file_path, os.path.join(bad_data_directory, os.path.basename(file_path)))\n",
    "\n",
    "    if os.path.isfile(file_path) and corrupted_ratio != 0:\n",
    "        os.remove(file_path)\n",
    "        print(\"File deleted successfully\")\n",
    "    elif corrupted_ratio == 0:\n",
    "        print(\"File is not corrupted and has been move to good data directory already\")\n",
    "    else:\n",
    "        print(\"Error: %s file not found\" % file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the selected file: ../data/raw_data/test_part_76.csv\n",
      "Corrupted Ratio = 0.0010449320794148381\n",
      "Good data has been moved to: ../data/good_data/good_data_1711449514.233256.csv\n",
      "Bad data has been moved to: ../data/bad_data/bad_data_1711449514.233256.csv\n",
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "data_validation(raw_data_directory='../data/raw_data', good_data_directory='../data/good_data', bad_data_directory='../data/bad_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
